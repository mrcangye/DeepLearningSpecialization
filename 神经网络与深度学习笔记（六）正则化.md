# 神经网络与深度学习笔记（六）正则化

## 前言

前面提到过高方差问题主要的两种方式：

- 获取更多的数据去训练。然而这种方式局限在于，数据并不是总是很容易获得的或者数据获取的代价很大。
- 正则化。这就是这篇文章需要来讨论的主题。

## 最小化代价函数正则化

使用 $L_2$ 正则化的最小化代价函数：
$$
min_{(w,b)}  \jmath(w,b) = \frac{1}{m}\sum_{i=1}^m\jmath(\hat y^{(i)},y^{(i)}) + \frac{\lambda}{2m} \Arrowvert w\Arrowvert^{2}_{2}
$$
其中，$\lambda$ 称为正则化参数。在编程过程中，常常把 $\lambda$ 写为`lambd`，以防止和`Python`语言中的关键字`lamba`起冲突。这一参数通常需要调优，使用开发集或者交叉验证集来验证。

那么：
$$
\Arrowvert w\Arrowvert^{2}_{2}
$$
到底是啥意思？

实际上，$\Arrowvert w\Arrowvert^{2}_{2}$ 被称为 $L_{2}$ 正则化，又称为参数 $w$ 的欧几里得范数，$L_2$ 范数等
$$
\Arrowvert w\Arrowvert^{2}_{2} = \sum_{j=1}^{n_x}w_j^{2}=w^Tw
$$
既然有 $L_2$ 范数，那么 $L_1$ 范数是啥？

$L_1$ 范数即为：
$$
\Arrowvert w\Arrowvert_{1} = \lvert w_1\lvert + \lvert w_2\lvert + \lvert w_3\lvert + \cdots \lvert w_n\lvert
$$
与此对应的 $L_1$ 正则化
$$
\sum_{j=1}^{n_x} \lvert w_j\lvert = \Arrowvert w\Arrowvert_{1}
$$
使用 $L_1$ 正则化的最小代价函数为：
$$
min_{(w,b)}  \jmath(w,b) = \frac{1}{m}\sum_{i=1}^m\jmath(\hat y^{(i)},y^{(i)}) + \frac{\lambda}{2m} \Arrowvert w\Arrowvert_{1}
$$
然而使用 $L_1$ 正则化的效果并不是太明显，主要是因为使用后会导致 $w$ 稀疏，$w$ 矢量中会有很多 0，虽然模型会有一定的压缩但是效果不大。

这就是为什么通常使用 $L_2$ 正则化而不是 $L_1$ 正则化的原因。

那么， $b$ 参数是否可以正则化呢？比如： $\frac{\lambda}{2m}b^2$

答案也是效果不大。

因为参数实际上大多数集中在 $w$ 中，而不是 $b$ ，即使对 $b$ 进行了正则化，$b$ 对模型的影响效果也不是太大

## 在神经网络中的 $L_2$ 正则化

$$
\begin{align}
\jmath (w^{[1]},b^{[1]},\cdots w^{[l]},b^{[l]}) &= \frac{1}{m} \sum_{i=1}^{m}L(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{i=1}^l \Arrowvert w^{[l]} \Arrowvert^{2}\\
\Arrowvert w^{[l]} \Arrowvert^{2} & = \sum_{i = 1}^{n^{[l]}} \sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2
\\
w^{[l]}&:(n^{[l]},n^{[l-1]})\\
\end{align}
$$

 

其中矩阵的 $\Arrowvert w^{[l]} \Arrowvert^{2} = \sum_{i = 1}^{n^{[l]}} \sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2$ ，表示矩阵中元素平方和。又称为弗罗贝尼乌斯范数（Frobenius norm） ，这里就不叫 $L_2$ 范数了。（为啥？我也不知道，而且弗罗贝尼乌斯范数不是对应元素的平方和再开方嘛？为什么这里不开方？）

在梯度下降的过程中,$dw^{[l]}$，$w^{[l]}$也会变化
$$
\begin{align}
dw^{[l]} & = dz^{[l]} * a^{[l-1]} + \frac{\lambda}{m}w^{[l]}\\
w^{[l]}& = w^{[l]} - \alpha dw^{[l]}\\
& = w^{[l]} - \alpha *(dz^{[l]} * a^{[l-1]} + \frac{\lambda}{m}w^{[l]})\\
& = w^{[l]} -\frac{\alpha \lambda}{m}w^{[l]} - \alpha(dz^{[l]} * a^{[l-1]})\\
& = w^{[l]}(1-\frac{\alpha \lambda}{m}) - \alpha(dz^{[l]} * a^{[l-1]})\\
\end{align}
$$
故，在梯度下降过程中，$w$ 是逐渐变小的，所以 $L_2$ 正则化有时称之为权重衰减



