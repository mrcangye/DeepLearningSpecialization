# 神经网络与深度学习笔记（二）正向传播与反向传播

## 正向传播

正向传播计算的是神经网络的输出

![](https://gitee.com/mrcangye/MyPictureBed/raw/master/img/%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD.png)

如上图，就是一次类似的正向传播的过程，正向传播计算最后的输出值。

将$J(a,b,c) = 3(a + b * c)$这一个式子用$u$, $v$ 来代替：

$u = b * c$

$v = a + u$

$j = 3 * v$

最后求出$j$的值

## 反向传播

反向传播计算神经网络的梯度以及微分

![](https://gitee.com/mrcangye/MyPictureBed/raw/master/img/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.png)

如上图是一个类似的反向传播的过程。

对图中右侧的输出 $j = 3 * v$ 我们有
$$
\frac{dj}{dv} = 3
$$
接着我们从右往左继续计算 $v = a + u$

对于 $v = a + u$ 有
$$
\frac{dj}{da} = \frac{dj}{dv} * \frac{dv}{da} = 3
$$
以及
$$
\frac{dj}{du} = \frac{dj}{dv} * \frac{dv}{du} = 3
$$
最后到达 $u = b * c$ 
$$
\frac{dj}{db} = \frac{dj}{dv} * \frac{dv}{du} * \frac{du}{db} = 3c
\\
\frac{dj}{dc} = \frac{dj}{dv} * \frac{dv}{du} * \frac{du}{dc} = 3b
$$
最后就得出了$\frac{dj}{da},\frac{dj}{db},\frac{dj}{dc}$的值

实际上这就是我们高等数学中的链式法则。通过求出上述三个的导数来计算梯度，进行梯度下降来更新 $w$ 与 $b$ 等参数

## 矢量计算

在计算神经网络的时候，我们尽量使用矢量来计算参数。因为`numpy`的计算速度一般是比使用`for`循环快的。

![矢量计算](https://gitee.com/mrcangye/MyPictureBed/raw/master/img/%E7%9F%A2%E9%87%8F%E8%AE%A1%E7%AE%97.png)

如上图，我们的输入变量有 $x_1,w_1,x_2,w_2,b$ 。如果使用普通的方法计算就显得十分繁琐

我们应该积极使用矢量化来简化计算速度

对于 $z$ 有 $z = w^{T}x + b$

其中我们知道:
$$
\mathbf{w}=
\left.
\begin{bmatrix}
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\end{bmatrix}\right\}_{n_x}
\mathbf{x}=
\left.
\begin{bmatrix}
\vdots \\
\vdots \\
\vdots \\
\vdots \\
\end{bmatrix}\right\}_{n_x}
$$
$w$ 与 $x$ 都是 $n_x$ 维

这是我们可以使用`numpy`这样写

```python
z = np.dot(w.T,x) + b
```

上面的代码段计算 $w$ 的转置和 $x$ 的乘积

同样的，对于：
$$
A = [a^{(1)},a^{(2)},\cdots a^{(m)}]\\
Y = [y^{(1)},y^{(2)},\cdots y^{(m)}]\\
dz = [dz^{(1)},dz^{(2)},\cdots dz^{(m)}] = A - Y = [a^{(1)}-y^{(1)},\cdots \cdots]\\
$$
也可进行类似的计算
$$
db = \frac{1}{m} \sum_{i}^mdz^{(i)} = \frac{1}{m}np.sum(dz)\\
\\
dw = \frac{1}{m}dz^T = \frac{1}{m} \begin{bmatrix}
\vdots  & \vdots & \vdots & \vdots \\
\vdots  & \vdots & \vdots & \vdots \\
x^{(1)} & \cdots & \cdots & x^{(m)} \\
\vdots  & \vdots & \vdots & \vdots \\
\vdots  & \vdots & \vdots & \vdots \\
\end{bmatrix} 
\begin{bmatrix}
\vdots\\
\vdots\\
dz^{(i)}\\
\vdots\\
\vdots \\
\end{bmatrix} = \frac{1}{m} \begin{bmatrix} x^{(1)}dz^{(1)} & \cdots & \cdots & x^{(m)}dz^{(m)} \\\end{bmatrix}
$$
Tips:

- `A.sum(sxis=0)#按列求和`

- 不要使用秩为1的数组。应类似于(5,1)而不是(5,)

  ```python
  a = np.random.randn(5)#错误
  a.shape(5,)
  a = np.random.randn(5,1)#正确
  ```

- 善于使用`assert`断言

  ```python
  assert(a.shape==(5,1))
  a.reshape((5,1))
  ```

  

## cost function由来

我们知道 cost 函数的式子是下面公式所示：
$$
\jmath(w,b) = - \frac{1}{m}\sum_{1}^m \left[ y^{(i)}log \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})\right]
$$
那么，为什么要这样定义？

因为我们要实现：

当 $y = 1$ 时：$P(y | x)=\hat y$

当 $y=0$ 时：$P(y|x)=1-\hat y$

在这种情况下，$P(y|x)$ 可以写成
$$
P(y | x) = \hat y ^{y} * ( 1 - \hat y )^{(1-y)}
$$


两边加对数得：
$$
logP(y|x) = ylog \hat y +(1-y)(log1-\hat y)
$$

## 神经网络层每层向量的形状



![神经网络层](https://gitee.com/mrcangye/MyPictureBed/raw/master/img/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82.png)

如上图，我们示例一个2层的神经网络。正如上图所示，神经网络的层数一般不包括输入层。计算层数时，一般将输入层视为第0层，然后依次往下数。在输入层与输出层之间是隐藏层。

接下来我们计算每层在运算时，矢量形状的变化

我们需要的还是以下的方程式
$$
z = wx + b\\
a = \sigma (z)
$$
约定：上标`[]`表示不同的层，上标`()`表不同的向量，下标`()`表向量形状

我们开始计算输入层到隐藏层：

有3个 $x_{(i)}$ 变量，故 $x$ 的形状为(3,1)

隐藏层有4个单元，故 $w$ 的形状为(4,3)

由 $w$ 与 $x$ 相乘的结果知道相乘后的形状为(4,1)，故b的形状为(4,1)，z的形状为(4,1)

综合有：
$$
z^{[1]}_{(4,1)} = w^{[1]}_{(4,3)}x_{(3,1)} + b^{[1]}_{(4,1)}\\

\\
a^{[1]}_{(4,1)} = \sigma (z^{[1]}_{(4,1)})
$$
继续到隐藏层到输出层的计算：

由输入层到隐藏层的计算结果可知，隐藏层的输入形状为(4,1)

而输出层只有一个神经元

故有：
$$
z^{[2]}_{(1,1)} = w^{[2]}_{(1,4)}a^{[1]}_{(4,1)}+b^{[2]}_{(1,1)}
\\
a^{[2]}_{(1,1)} = \sigma(z^{[2]}_{(1,1)})
$$
因此我们推广开可以得到：
$$
z^{[l]} = w^{[l]}a^{[l-1]}+b^{[l]}
\\
a^{[l]} = \sigma(z^{[l]})
$$
上面就是前向传播每层单元的关系

我们设 $n^{[l]}$ 为第 $l$ 层的单元数

则：
$$
w^{[l]}与 dw 的形状均为：(n^{[l]},n^{[l-1]})\\
b^{[l]}与 db 的形状均为：(n^{[l]},1)
$$
